基于视频分析与理解的高级视觉任务

行为识别Action Detection概述及资源合集（持续更新...）
https://blog.csdn.net/gavinmiaoc/article/details/81179630
https://blog.csdn.net/c2a2o2/article/details/82896285

视频动作检测 识别  action detection recognition

视频描述 video caption(description)
----------------------------------------------------------
视频动作检测 识别  action detection recognition
Human action  具有特定语义的人类物体的动作集合

Action analysis

应用：

给视频贴标签

城市安防

人机智能交互


主要数据集

UCF 101 

困难：

1 同一类可能有很大的差别
2 两帧视频的区别可能会很大
3 拍摄摄像头的运动
4 背景出现和前景相似的纹理或特征
5 遮挡和光照变化


传统方法
特征提取 ----》 编码  -----》词袋模型 ----》SVM分类

两针视频叠加 +  光流 （前景光流） +   轨迹


词袋模型：
人偶   自行车  书籍

提取特征

聚类

（准备进行判别）


未知图片  --- 》提取特征 ------》 四个主要特征含有多大的比例  直方图 ----》分类


1. 怎么获取更好的特征
2. 如何来对特征进行压缩和重合


基于深度学习的动作分析方法：

双路模型

forves stream :center crop of the original frame 原始框架的中心裁剪
               视网膜的中心   接收的输入就是图像中心的一块区域

context stream ：resize of the original frame 调整原始框架的大小


几种特征融合方法：

single frame:单帧的图像输入到网络中去  单帧分类的平均

late fusion ：晚期融合  在获取单帧图像特征之后 隔一段时间  再获取一帧图像特征

early fusion : 将几帧图像融合 形成 N * 3(RGB)的特征

slow fusion :  c3d


评比正确：HIT @ 5  取前五个正确率




利用光流表征运动信息：


spatial stream convnet :接收RGB图像的输入


temporal stream convnet :接收叠加的光流场的输入

两个平均 置信度


运动的表征方式？

滤波器可视化



改进特征融合方法：

空间纬度融合

sum fusion  相加

max ..      对每个空间特征取两个通道取最大

concatenation fusion   叠加

conv fusion  卷积 线性加权

bilinear fusion  两个向量的外积


时间纬度融合：

3d卷积   3D池化


网络整体结构：

----------------------------------------------
视频描述 video caption(description)

视频  ----》文字描述

应用场景：

应用视频检索：

构建特征空间   语言特征空间

视频描述服务


主要数据集：


评价指标：

端到端的视频描述模型：
input video ---> convolution  net（CNN）  -----> recurrent net（rnn）  -----> output

LSTM:很好的建模数据间的关系

LOSS 定义：  
cross-entropy loss 来优化模型

------------------------------------------------
基于tensorflow 框架的视频描述模型：

S2VT:
--------------------------------------------------------------------
图像分类：


 



---------------------------------------------------------------------
人脸识别算法：

问题：预防欺诈能力 -----》活体检测

（1）1:1的场景：

（2）1：N的场景：

（3）智能寻亲场景：

------
（1）模板匹配
N*N IMAGE  N^2 vector   图片向量
问题：（1）维度灾难     （2）准确率问题

问题转化为：（将向量映射到低维空间  ---特征 ）在低维特征空间同一个人的特征尽可能接近
            不同人的特征尽可能分离

128维特征


如何找到低维空间呢？
前深度学习时代：主成分分析 ：我找到低维空间  距离最大的是主成分 找一条直线映射  尽可能让
		数据分开
		eigen face 特征脸
		（1）计算平均脸
 		（2）将训练集中的人脸图像减去平均人脸图像
		（3）求协方差矩阵

		特征脸问题：
		1.特征表达能力有限
		2.适合小型数据库



deep learning时代：

人脸识别流程 ――》检测-》对其->特征提取-》识别

（1）人脸检测
两种解决方法：    画框 --- 分类器 ----得到最终分类
    1. 回归方法  I 输入图片图片 (反解)----》   [x,y,w,h]  画框
    2. Proposal（滑窗） + 分类 + 合并
       Proposal  + 分类（多尺度） -----》CNN ---->分类（得到许多含有人脸的区域）  
		-----》合并 非极大值的抑制  

       非极大值的抑制: 
		       1 先将所有的框的得分进行排序  选中最高分及其对应的框
		       2 遍历其余的框  如果和当前最高分框的重叠面积（IOU）大于一定阈值，
		         我们就将它删除
		       3 从未处理的框中继续选中一个得分最高的，重复上述过程


（2）人脸对齐
  如何对齐：确定人脸中标定点的位置

  三个点-----仿射变换：
		       （1）二维坐标到二维坐标之间的线性变换
		       （2）不公线的三对对应点决定了一个唯一的仿射变换

  关键点寻找---------》回归

  级联的思想

特征学习；
   分类模型：
      deepface
      deepid
      vgg
      resnet 


   度量学习模型：
                   

（3）人脸验证：
    1:1 ：欧氏距离  和 余弦距离   ，joint bayesian 方法更具性能
     

---------------------------------------------------------------------
图像分割和语义图像分割

	语义分割：像素基本分类

	实例分割：像素基本分类以外，还要区分每个物体

分割评价好坏：

	IOU    预测结果和label的重合程度 

	平均IOU 


深度学习解决分割问题：
	
	F:全卷积网络（FCN）  输出 W* H* K的概率矩阵

	1  海量数据和标注
        2  合理的LOSS定义  CROSS-ENTROPY  L1 L2
        3  优化方法  Adam SGD

	image segmentation的loss不是一个标量，二是一个矩阵

	FCN结构  


==========================================================================================
序列分析：

图像分析：one to one 
图片标注：one to many
动作识别：many to one
机器翻译，视频标注（解说）：many to many
动作识别（帧级别）：many to many

序列分析: 多个输入或者多个输出

使用循环神经网络（RNN）:

1 结构  2 反向传播 3 问题：梯度消失  长期依赖问题（前面的输入对后面的影响较小）

-------------
LSTM

(1)LSTM网络  详细内容

(2)Attention LSTM:
在LSTM网络的基础上添加注意力机制：只对关注的地方提取特征

感受野：卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上的映射的区域大小

在feature map 学习权值



(3)ConvLSTM: 空间LSTM

--------------
C3D: 3D卷积 时间轴 +  当前图像


--------------

光流：空间运动物体在观察成像平面上的像素运动的瞬时速度


计算方法： 
1 寻找上一帧跟当前帧之间存在的对应关系
2 计算出相邻帧之间物体的运动信息


很容易区分运动物体和不运动的物体



动作识别 ：  帧+光流 


光流问题：计算缓慢  

FLowNet




 








