深度学习 常用符号
http://www.ai-start.com/dl2017/html/notation.html





二分类(Binary Classification)



为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分?

使用了逻辑回归(logistic regression)： 逻辑回归是一个用于二分类(binary classification)的算法

逻辑回归的代价函数（Logistic Regression Cost Function ）或 成本函数：Loss function:L(y^,y)


损失函数：损失函数又叫做误差函数，用来衡量算法的运行情况

在这门课中有很多的函数效果和现在这个类似，就是如果y等于1，我们就尽可能让y^变大，如果y等于0，我们就尽可能让y^变小



为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，
算法的代价函数是对个样本的损失函数求和然后除以:  损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价



L成本函数 =  单个样本的损失函数之和  / M

怎么样控制L 降到最低呢？

梯度下降法（Gradient Descent）   --------》  导数 就是斜率



计算图（Computation Graph）  计算公式 不是结果
首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作： 计算图解释了为什么我们用这种方式组织这些计算过程。
计算图组织计算的形式是用蓝色箭头从左到右的计算

 使用计算图求导数（Derivatives with a Computation Graph）  -------------》微积分里这叫链式法则   输出变量对某个变量的导数，我们就用命名dvar
 在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关心的变量对的导数，那么我们就做完了一步反向传播

 
 
 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）：
 
 10 m 个样本的梯度下降(Gradient Descent on m Examples)
 
( L成本函数 =  单个样本的损失函数之和  / M)   计算各个修正值后 需要 for循环遍历所有样本的 成本函数  ，如果数据量巨大的话 就会出现问题


所以引出：


 向量化(Vectorization)  向量化是非常基础的去除代码中for循环的艺术
 
 
 
 
 

(1)前向暂停(forward pause)或叫做前向传播(foward propagation)

(2)反向暂停(backward pause) 或叫做反向传播(backward propagation)





神经网络的表示（Neural Network Representation）


a 代表激活值  ：
输入层的激活值位  a[0]   

隐藏层的激活值  a[1]    a1[1]  a2[1]....... 不同节点的激活值    

输出层将产生某个数值a，它只是一个单独的实数 y^值将取为 a[2]




隐藏层 ：拥有W  和  b  参数   w[1]b[1]  w 4*3  b 4*1
输出层： 拥有w  和  b  参数   w[2]b[2]  w 1*4  b 1*1




计算一个神经网络的输出（Computing a Neural Network's output）
x表示输入特征，
a表示每个神经元的输出，
W表示特征的权重，
上标表示神经网络的层数（隐藏层为1），
下标表示该层的第几个神经元




多样本向量化（Vectorizing across multiple examples）

向量化实现的解释（Justification for vectorized implementation）
矩阵乘以列向量得到列向量



深层神经网络（Deep L-layer neural network）


前向传播和反向传播（Forward and backward propagation）

每一层都有前向传播步骤以及一个相反的反向传播步骤


深层网络中的前向传播（Forward propagation in a Deep Network）

核对矩阵的维数（Getting your matrix dimensions right）
在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的




为什么使用深层表示？（Why deep representations?）


深度网络在计算什么：
如果你在建一个人脸识别或是人脸检测系统，深度神经网络所做的事就是，当你输入一张脸部的照片，
然后你可以把深度神经网络的第一层，当成一个特征探测器或者边缘探测器。



偏差，方差（Bias /Variance）

可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。



正则化（Regularization）： 解决正则化





 












