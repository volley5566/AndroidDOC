二分类(Binary Classification)



为什么神经网络的训练过程可以分为前向传播和反向传播两个独立的部分?

使用了逻辑回归(logistic regression)： 逻辑回归是一个用于二分类(binary classification)的算法

逻辑回归的代价函数（Logistic Regression Cost Function ）或 成本函数：Loss function:L(y^,y)


损失函数：损失函数又叫做误差函数，用来衡量算法的运行情况

在这门课中有很多的函数效果和现在这个类似，就是如果y等于1，我们就尽可能让y^变大，如果y等于0，我们就尽可能让y^变小



为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，
算法的代价函数是对个样本的损失函数求和然后除以:  损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价



L成本函数 =  单个样本的损失函数之和  / M

怎么样控制L 降到最低呢？

梯度下降法（Gradient Descent）   --------》  导数 就是斜率



计算图（Computation Graph）  计算公式 不是结果
首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作： 计算图解释了为什么我们用这种方式组织这些计算过程。
计算图组织计算的形式是用蓝色箭头从左到右的计算

 使用计算图求导数（Derivatives with a Computation Graph）  -------------》微积分里这叫链式法则   输出变量对某个变量的导数，我们就用命名dvar
 在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关心的变量对的导数，那么我们就做完了一步反向传播

 
 
 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）：
 
 10 m 个样本的梯度下降(Gradient Descent on m Examples)
 
( L成本函数 =  单个样本的损失函数之和  / M)   计算各个修正值后 需要 for循环遍历所有样本的 成本函数  ，如果数据量巨大的话 就会出现问题



 
 
 
 
 
 

(1)前向暂停(forward pause)或叫做前向传播(foward propagation)

(2)反向暂停(backward pause) 或叫做反向传播(backward propagation)







